{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LSPE Equations](./images/table-3.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GE-LSPE: Geometrically Enhanced Learnable Structural and Positional Encodings\n",
    "\n",
    "For a better read, please refer to report.pdf in the repository. \n",
    "\n",
    "## Introduction\n",
    "\n",
    "Graph Neural Networks (GNNs), dating back to the 1990s, [Sperduti, 1993, Gori et al., 2005,\n",
    "Bruna et al., 2014, Kipf and Welling, 2016 ] have ascended from a niche of representation learn-\n",
    "ing to one of its most successful methods. GNNs are commonly used for analyzing and processing\n",
    "graph data, as they have the capability to learn using local information and, to some extent, the\n",
    "global structural information of a graph. Their applications span a wide range of domains, from\n",
    "recommender systems [Ying et al., 2018] to drug discovery [Han et al., 2021].\n",
    "\n",
    "A rather significant role in the success of GNNs is partially attributed to the message-passing\n",
    "framework [Gilmer et al., 2017a ], which enables nodes in a graph to exchange and aggregate in-\n",
    "formation through iterative message passing steps, enhancing their representation and facilitating\n",
    "effective learning on graph-structured data. However, it has been shown that this framework\n",
    "suffers from fundamental limitations [ Xu et al., 2018 ], which constrain their expressivity. More\n",
    "specifically, GNNs are shown to be as powerful as the 1-dimensional Weisfeiler-Lehman (WL) test\n",
    "[ Weisfeiler and Leman, 1968] in distinguishing non-isomorphic (sub-)graphs [ Morris et al., 2018].\n",
    "Due to the locality of the message-passing framework, each node in GNNs only aggregates from its\n",
    "direct neighbours. This local view can be a drawback when trying to learn global graph properties or\n",
    "when the important features of a node are dependent on nodes beyond its immediate neighbourhood.\n",
    "\n",
    "Recent research has been dedicated to enhancing the discriminative capabilities of GNNs, pushing\n",
    "past the constraints imposed by the 1-WL test. One solution to this issue involves providing\n",
    "inductive biases to the GNNs in the form of the data’s geometric information [ Satorras et al., 2021,\n",
    "Brandstetter et al., 2021]. While incorporating the distance norm improves the model’s performance,\n",
    "it still suffers from limitations in expressivity by not being able to learn explicit higher-dimensional\n",
    "features present in the graph. A different line of research focuses on providing this information through\n",
    "topology by integrating Positional Encodings (PE) such as Random Walk-based [ Bourgain, 1985 ,\n",
    "Chung, 1997 ] or Laplacian Eigenvector-based Encodings [Dwivedi et al., 2022 ]. These techniques\n",
    "aim to capture more global information and provide a richer representation of the graph beyond\n",
    "immediate neighbourhood interactions. Another more recent approach involves using Learnable\n",
    "Structural and Positional Encodings (LSPE) [Dwivedi et al., 2021 ] to decouple the structural (node\n",
    "features) and positional (node’s position within the graph) representations within GNNs, allowing\n",
    "them to be learned independently and leading to an increased expressivity and performance.\n",
    "To further enhance the expressive power of GNNs, this research project takes inspiration from the\n",
    "Equivariant Message Passing Simplicial Network (EMPSN) architecture [Eijkelboom et al., 2023], a\n",
    "novel approach that combines geometric and topological information on simplicial complexes. Our\n",
    "goal is to develop a generic method that also combines geometric and topological information by\n",
    "improving upon the established LSPE framework. By combining these distinct approaches, we seek to leverage the complementary nature of geometric and topological information in capturing complex graph relationships and enhancing the discriminative capabilities of GNN models. We highlight the\n",
    "following contributions:\n",
    "\n",
    "\n",
    "- We demonstrate that smaller, less complex models experience larger performance gains when utilizing PEs.\n",
    "\n",
    "- We discover a relationship between model complexity and the impact of topological information, where less complex models benefit the most from this additional information.\n",
    "\n",
    "- Injecting topological information through PEs significantly improves performance in non-fully connected settings.\n",
    "\n",
    "- We find that topological information does not necessarily enhance the EGNN model in a fully-connected setting, suggesting that the model learns topology through geometry.\n",
    "\n",
    "- By adapting the LSPE framework to include Geometry, we create a generic method to incorporate both geometric and topological information.\n",
    "\n",
    "- We improve the performance of the standard EGNN model in non-fully connected scenarios.\n",
    "\n",
    "- Our method demonstrates its applicability to different architectures with sufficient model\n",
    "complexity, revealing the limitations of the LSPE framework for less complex models.\n",
    "\n",
    "## Related Works\n",
    "\n",
    "### Message Passing Neural Networks\n",
    "\n",
    "Message Passing Neural Networks (MPNNs) have emerged as a popular class of models for graph\n",
    "representation learning. MPNNs allow nodes in a graph to exchange information with their\n",
    "neighbouring nodes through message-passing iterations, enabling the incorporation of local graph\n",
    "structure into node representations. These models typically consist of two key steps: message\n",
    "propagation, where each node aggregates information from its neighbours, and node update, where\n",
    "the aggregated information is used to update the node’s representation. MPNNs have demonstrated\n",
    "strong performance in various graph-related tasks, including node classification, link prediction, and\n",
    "graph generation. Several notable MPNN variants include Graph Convolutional Networks (GCNs)\n",
    "[Kipf and Welling, 2016 ], Graph Attention Networks (GATs) [ Veliˇckovi ́c et al., 2017 ], GraphSAGE\n",
    "[ Hamilton et al., 2017] and Graph Isomorphism Networks (GIN) [Xu et al., 2018 ], each offering\n",
    "unique strategies for message passing and aggregation.\n",
    "\n",
    "![MPNN equations](./images/MPNN%20equations.png)\n",
    "\n",
    "\n",
    "### E(n) Equivariant GNN\n",
    "\n",
    "EGNNs [Satorras et al., 2021] extend MPNNs by relying on graph geometry to improve performance\n",
    "and attain E(n) equivariance. One particular aspect that renders EGNNs highly effective in capturing\n",
    "graph-level topology yet computationally expensive is that they operate best in fully-connected\n",
    "settings with a moderately deep architecture. However, this approach might not always be feasible,\n",
    "as the number of layers needed to learn higher dimensional graph structures scales exponentially,\n",
    "which ultimately renders such architectures intractable for large graphs.\n",
    "\n",
    "### Positional Encodings\n",
    "\n",
    "Positional encodings (PE) are a fundamental concept that significantly influences the effective-\n",
    "ness of many network architectures, including CNNs [Lecun et al., 1998 ], RNNs [Hopfield, 1982 ,\n",
    "Hochreiter and Schmidhuber, 1997], and most recently, Transformers [Vaswani et al., 2017], by\n",
    "providing a means to infuse positional information into deep learning models. However, infer-\n",
    "ring nodes’ positions in any given graph is a non-trivial task due to the absence of a canon-\n",
    "ical positioning method for capturing both local and global information. While GNNs have\n",
    "been shown to outperform traditional algorithms for node classification, subpar performance\n",
    "was demonstrated in [Hu et al., 2020 ] when compared to simple heuristics such as Adamic Adar\n",
    "[Adamic and Adar, 2003] on link prediction tasks [ Liben-Nowell and Kleinberg, 2003 ]. Recent work\n",
    "[ Srinivasan and Ribeiro, 2019, Br ̈uel-Gabrielsson et al., 2022 , Wang et al., 2022] have (empirically)\n",
    "rendered the addition of PE in GNNs crucial in achieving state-of-the-art (SOTA) in graph predic-\n",
    "tion tasks. Several candidates for PE have been proposed, such as Index PE [Murphy et al., 2019 ],\n",
    "Laplacian Eigenvectors [Dwivedi et al., 2022 ] and learnable position-aware embeddings based on\n",
    "random anchor node sets [You et al., 2019]. Another relevant method which this study focuses on\n",
    "involves diffusion-based Random walks [ Bourgain, 1985 , Chung, 1997]. The encodings produced\n",
    "with this method carry significant descriptive power when considering graph topology, as they can\n",
    "effectively capture the graph’s diffusion characteristics [ Topping et al., 2022]. Formally, the RW\n",
    "matrix can be defined over k-steps as :\n",
    "\n",
    "![PE vector](./images/PE-vector.png)\n",
    "\n",
    "### Learnable Structural and Positional Encodings\n",
    "\n",
    "Learnable Structural and Positional Encodings (LSPE) [ Dwivedi et al., 2021 ] have been proposed as\n",
    "an extension to traditional GNN architectures. LSPE combines structural and positional encodings\n",
    "to learn expressive representations that better capture both local and global graph information,\n",
    "resulting in more expressive node embeddings. The structural encoding component focuses on\n",
    "connectivity patterns and neighbourhood information, while the positional encoding component\n",
    "provides topological information into the node representations.\n",
    "Incorporating LSPE into GNNs offers several benefits. It enhances the GNN’s capability to\n",
    "better capture both local and global graph characteristics, enabling better handling of complex\n",
    "graph structures. The learnable nature of the encodings allows the model to adapt and optimize\n",
    "representations for specific tasks, making it suitable for diverse graphs with varying structural and\n",
    "positional properties. The combination of structural and positional encodings provides a richer\n",
    "representation of the graph, manifested through more expressive node embeddings, leading to\n",
    "improved performance in node classification, link prediction, and graph generation tasks.\n",
    "\n",
    "![LSPE Equations](./images/LSPE-equations.png)\n",
    "\n",
    "\n",
    "\n",
    "## Methodology\n",
    "\n",
    "Recognizing the significant role of node distances in capturing the graph’s topology within the\n",
    "original EGNN architecture and the promising results of the LSPE framework as mentioned in\n",
    "Subsection 2.4, we propose a method which combines these two techniques. By integrating the\n",
    "geometrical features of the graph (node distances in the case of QM9) with topological features\n",
    "given by PEs, we seek to achieve more expressive node attributes.\n",
    "\n",
    "### GeTo-MPNN\n",
    "\n",
    "As mentioned in Section 2, EGNNs have achieved remarkable success by using the underlying\n",
    "geometry of the graph space to their advantage. However, it is important to note that the\n",
    "computational cost associated with fully connected graphs can limit the feasibility of this approach.\n",
    "As the dimensionality of the graph structures increases (e.g. cycles), the number of layers required\n",
    "to learn such higher-dimensional structures scales exponentially. For that reason, we first aim to\n",
    "explore the benefits of incorporating implicit topology in the EGNN framework. This is performed\n",
    "by using a random walk encoding (k = 24), explained in Section 2.2, by embedding the hidden\n",
    "features of a node together with its positional encoding to a higher dimensional space through\n",
    "concatenation.\n",
    "Our method proposes to combine the LSPE method while also making use of the geometrical\n",
    "information found in EGNNs, by taking relative absolute distances between nodes into account in\n",
    "the message function. The additional feature, the norm of distance in this case, is added through\n",
    "concatenation to the message network for both the node message and the PE message as can be\n",
    "seen in Table 1. One benefit of this method is that parameter count does not drastically increase\n",
    "when adding LSPE with Geometry as only one scalar attribute is added to each MPNN layer.\n",
    "\n",
    "### MPNN Layers\n",
    "\n",
    "Two different MPNN architectures were examined in this study. The first one, which this study\n",
    "refers to as Standard MPNN, calculates the messages based on both receiving and sending nodes\n",
    "whereas the second architecture, the Isotropic MPNN [ Tailor et al., 2022], only uses sending nodes\n",
    "to calculate messages, allowing us to test our method on less expressive models.\n",
    "In order to quantify the contribution of geometry in the LSPE framework, we run a set of experiments\n",
    "on 6 variants. These include the basic MPNN model, adding Geometry only (for which the Standard\n",
    "MPNN with Geometry resembles the EGNN), PE only, PE and Geometry, LSPE and LSPE with\n",
    "Geometry. The last one being our proposal described in 3.1. For all the aforementioned models\n",
    "including PEs, we used a Random Walk (RW) diffusion-based positional encoding scheme as described\n",
    "in Section 2.2. The detailed formulas for each model can be found in Tables 1 and 2.\n",
    "\n",
    "\n",
    "\n",
    "## Results and Analysis\n",
    "\n",
    "In this section, we will first examine how infusing the models with implicit topological information\n",
    "in the shape of Random Walk PEs (RWPE) affects their performance on the QM9 dataset in a\n",
    "fully connected (FC) and non-fully connected (NFC) setting. Moreover, we will demonstrate how\n",
    "geometrical information, the absolute distance between nodes, can be utilized effectively to learn\n",
    "better node embeddings.\n",
    "In our first experiment, as mentioned in Subsection 2.3, we evaluated the EGNN using both\n",
    "original edges, and fully connected ones, with and without infusing the Positional Encodings. When\n",
    "comparing the performance for the different kinds of connections between the nodes, it can be\n",
    "seen from Table 4, that the performance when all the nodes are connected is better in\n",
    "all experiments. One reason for this boost in performance could be attributed to the EGNN\n",
    "architecture using geometry to learn the important edges. Also, as expected, in all of our results, it\n",
    "can be seen that more layers result in a higher performance, which is explained by the fact that\n",
    "each layer can gradually gather and incorporate information from neighbouring nodes and edges,\n",
    "capturing information from different hops in the graph.\n",
    "Furthermore, to examine the trade-off between model complexity and the contribution of the\n",
    "topological information when embedded together with the hidden state, different numbers of layers\n",
    "were used for evaluation. From Table 4, it can be observed that the effect of PEs becomes more\n",
    "pronounced with a lower number of layers in the EGNN, suggesting that PEs play a crucial role\n",
    "in compensating for the challenges faced by shallower models in capturing distant topological\n",
    "information within the graph structure. On the other hand, as model complexity increases, the\n",
    "EGNN architecture does not particularly benefit from the use of PEs, especially in the FC setting. One possible explanation for this effect is that the EGNN model is able to implicitly learn\n",
    "topology by using the graph’s geometry. In addition, it is interesting to mention that the 1-layer\n",
    "EGNN with no PEs obtains a much better performance in a FC setting, which can be attributed to\n",
    "the fact that in the FC setting the EGNN can better capture long-range dependencies as the whole\n",
    "graph becomes accessible in one hop.\n",
    "\n",
    "![LSPE Equations](./images/table-3.png)\n",
    "\n",
    "For the second experiment, we trained both model architectures and all of their variants on the\n",
    "QM9 dataset for 4 and 7 layers in a NFC setting. The results are shown in table 4. The MPNNs\n",
    "introduced in Section 3.1 were used for these experiments. When analysing the effect of adding\n",
    "PEs, the performance for both Standard and Isotropic MPNN models increases, while for 7 layers\n",
    "the performance gained from using PEs is lessened. This can be related to the results from the\n",
    "first experiment, where a trade-off between model complexity and the contribution of topological\n",
    "information is experienced. This might suggest that the EGNN (MPNN-Geom) model uses\n",
    "geometry to learn topology. As for the combination of PE and Geometry through stacking\n",
    "(PE-Geom), we can observe how performance is always improved compared to only using PE with\n",
    "the 4-layer models. An interesting finding is that for the 7-layer models, performance gain actually\n",
    "increases with model complexity for the MPNN-Isotropic. This indicates that the trade-off point\n",
    "has not yet been reached and thus signifying that MPNN-Isotropic can utilize better the stacked PE\n",
    "with Geometry.\n",
    "When analysing the results for the models using LSPE, they show that the normal MPNN model\n",
    "always results in an increased performance while the opposite happens for the Isotropic MPNN. We\n",
    "thus hypothesise that the model is not complex enough to effectively use the learnt PEs.\n",
    "For LSPE with Geometry, we again see a similar effect as previously mentioned, with MPNN-LSPE-\n",
    "Geom achieving consistently the best performance for 4 and 7 layers. For the Isotropic MPNN, we\n",
    "experience a performance gain with respect to the base model, but infusing PEs (PE-Geom) still\n",
    "results in a better performance. It is worth mentioning that the performance gap between PE-Geom\n",
    "and LSPE-Geom becomes smaller for the Isotropic model as the number of layers increases, which supports our previous claim, that more complexity is needed to use the learnable encodings efficiently.\n",
    "Motivated by the previous findings, we also trained the MPNN-Isotropic architecture and its\n",
    "variants under the same conditions for 10 layers to examine whether our previous hypothesis will\n",
    "be supported. The results are shown in Table 5, where we can see how the LSPE-Geom model\n",
    "outperforms both the PE and PE-Geom models, and with the LSPE model resulting in\n",
    "the worst performance. This not only proves that more complexity was needed to better use the\n",
    "learnt encodings, but it also demonstrates how our method for integrating topology and geometry\n",
    "is applicable to different models, resulting in a better overall performance than just using LSPE\n",
    "without geometry.\n",
    "\n",
    "![LSPE Equations](./images/table-4-5.png)\n",
    "\n",
    "## Conclusion \n",
    "\n",
    "This work examines how injecting topological information into the EGNN model affects its perfor-\n",
    "mance in a fully connected and non-fully connected setting. Through an extensive investigation,\n",
    "we compared the original architecture with an enhanced version incorporating additional implicit\n",
    "topological information via Random Walk Positional Encodings. Our experiments revealed how,\n",
    "depending on the model’s size, using implicit topological information may be beneficial to the\n",
    "EGNN’s performance. Notably, these benefits were particularly prominent when using fewer layers\n",
    "in the EGNN, effectively addressing the challenge of capturing distant topological information\n",
    "encountered by shallower models. Furthermore, as the model complexity increased, the contribution\n",
    "of topological information diminished, suggesting a trade-off between model complexity and the\n",
    "incorporation of implicit topological knowledge in the EGNN architecture.\n",
    "Recognizing the value of additional topological information in enhancing structural representations\n",
    "and inspired from the benefits of EGNN and the LSPE method, this work proposes a generic\n",
    "approach to leverage geometry for improved positional encodings when utilizing LSPE. This method\n",
    "achieved the best performance when models have sufficient computational complexity to make use of\n",
    "this additional information indicating that geometry can be indeed utilized to learn better structural\n",
    "and positional encodings, thus obtaining better results.\n",
    "While our empirical study shows promising results in combining geometry and topology, several\n",
    "limitations in our study could benefit from further research. Future investigations should involve\n",
    "conducting experiments with a wider range of GNN architectures to expand our understanding\n",
    "of their interaction with topological information. Regarding the dataset, trying our method on\n",
    "inferring more tasks of the QM9 dataset could be beneficial.\n",
    "Using LSPE entails higher computational complexity, however, a proposition that could be further\n",
    "investigated is whether we can use less complex layers to learn the same PEs by applying geometry.\n",
    "This becomes particularly relevant as we observe the enhanced performance with increased layers.\n",
    "Moreover, our present method of conditioning, achieved exclusively through concatenation, could be\n",
    "extended to alternatives that could potentially offer more efficient or expressive results. Additionally,\n",
    "our methodology presents promising potential for application on datasets where topological informa-\n",
    "tion is valuable but it is computationally prohibitive to connect all nodes. Through the utilization\n",
    "of our approach, one could potentially achieve an equilibrium between computational feasibility and\n",
    "the use of topological information. Moreover, while the EGNN paper[Satorras et al., 2021] tests the\n",
    "model in the QM9 dataset with the nodes’ coordinates fixed, one could evaluate the same framework\n",
    "on datasets which coordinates are updated in order to examine the way that topological information\n",
    "(PEs) interacts with the coordinate update. Finally, considering the exceptional performance of the\n",
    "EGNN framework in fully connected settings without the need for LSPE, we believe that our method\n",
    "holds promise when applied to graph datasets where the fully connected setting is computationally\n",
    "prohibitive, potentially leading to favourable outcomes.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![\\\\ \\begin{equation} \\\\ \\label{eq:mpnn_1} \\\\     h_i^{\\ell+1} =f_h\\left(h_i^{\\ell},\\left\\{h_j^{\\ell}\\right\\}_{j \\in \\mathcal{N}_i}, e_{i j}^{\\ell}\\right), h_i^{\\ell+1}, h_i^{\\ell} \\in \\mathbb{R}^d,  \\\\ \\end{equation} \\\\  \\\\ \\begin{equation} \\\\ \\label{eq:mpnn_2} \\\\     e_{i j}^{\\ell+1} =f_e\\left(h_i^{\\ell}, h_j^{\\ell}, e_{i j}^{\\ell}\\right), e_{i j}^{\\ell+1}, e_{i j}^{\\ell} \\in \\mathbb{R}^d \\\\ \\end{equation}](https://latex.codecogs.com/svg.latex?%5C%5C%20%5Cbegin%7Bequation%7D%20%5C%5C%20%5Clabel%7Beq%3Ampnn_1%7D%20%5C%5C%20%20%20%20%20h_i%5E%7B%5Cell%2B1%7D%20%3Df_h%5Cleft(h_i%5E%7B%5Cell%7D%2C%5Cleft%5C%7Bh_j%5E%7B%5Cell%7D%5Cright%5C%7D_%7Bj%20%5Cin%20%5Cmathcal%7BN%7D_i%7D%2C%20e_%7Bi%20j%7D%5E%7B%5Cell%7D%5Cright)%2C%20h_i%5E%7B%5Cell%2B1%7D%2C%20h_i%5E%7B%5Cell%7D%20%5Cin%20%5Cmathbb%7BR%7D%5Ed%2C%20%20%5C%5C%20%5Cend%7Bequation%7D%20%5C%5C%20%20%5C%5C%20%5Cbegin%7Bequation%7D%20%5C%5C%20%5Clabel%7Beq%3Ampnn_2%7D%20%5C%5C%20%20%20%20%20e_%7Bi%20j%7D%5E%7B%5Cell%2B1%7D%20%3Df_e%5Cleft(h_i%5E%7B%5Cell%7D%2C%20h_j%5E%7B%5Cell%7D%2C%20e_%7Bi%20j%7D%5E%7B%5Cell%7D%5Cright)%2C%20e_%7Bi%20j%7D%5E%7B%5Cell%2B1%7D%2C%20e_%7Bi%20j%7D%5E%7B%5Cell%7D%20%5Cin%20%5Cmathbb%7BR%7D%5Ed%20%5C%5C%20%5Cend%7Bequation%7D)](#_)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
