{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GE-LSPE: Geometrically Enhanced Learnable Structural and Positional Encodings\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Graph Neural Networks (GNNs), dating back to the 1990s, [Sperduti, 1993, Gori et al., 2005,\n",
    "Bruna et al., 2014, Kipf and Welling, 2016 ] have ascended from a niche of representation learn-\n",
    "ing to one of its most successful methods. GNNs are commonly used for analyzing and processing\n",
    "graph data, as they have the capability to learn using local information and, to some extent, the\n",
    "global structural information of a graph. Their applications span a wide range of domains, from\n",
    "recommender systems [Ying et al., 2018] to drug discovery [Han et al., 2021].\n",
    "\n",
    "A rather significant role in the success of GNNs is partially attributed to the message-passing\n",
    "framework [Gilmer et al., 2017a ], which enables nodes in a graph to exchange and aggregate in-\n",
    "formation through iterative message passing steps, enhancing their representation and facilitating\n",
    "effective learning on graph-structured data. However, it has been shown that this framework\n",
    "suffers from fundamental limitations [ Xu et al., 2018 ], which constrain their expressivity. More\n",
    "specifically, GNNs are shown to be as powerful as the 1-dimensional Weisfeiler-Lehman (WL) test\n",
    "[ Weisfeiler and Leman, 1968] in distinguishing non-isomorphic (sub-)graphs [ Morris et al., 2018].\n",
    "Due to the locality of the message-passing framework, each node in GNNs only aggregates from its\n",
    "direct neighbours. This local view can be a drawback when trying to learn global graph properties or\n",
    "when the important features of a node are dependent on nodes beyond its immediate neighbourhood.\n",
    "\n",
    "Recent research has been dedicated to enhancing the discriminative capabilities of GNNs, pushing\n",
    "past the constraints imposed by the 1-WL test. One solution to this issue involves providing\n",
    "inductive biases to the GNNs in the form of the data’s geometric information [ Satorras et al., 2021,\n",
    "Brandstetter et al., 2021]. While incorporating the distance norm improves the model’s performance,\n",
    "it still suffers from limitations in expressivity by not being able to learn explicit higher-dimensional\n",
    "features present in the graph. A different line of research focuses on providing this information through\n",
    "topology by integrating Positional Encodings (PE) such as Random Walk-based [ Bourgain, 1985 ,\n",
    "Chung, 1997 ] or Laplacian Eigenvector-based Encodings [Dwivedi et al., 2022 ]. These techniques\n",
    "aim to capture more global information and provide a richer representation of the graph beyond\n",
    "immediate neighbourhood interactions. Another more recent approach involves using Learnable\n",
    "Structural and Positional Encodings (LSPE) [Dwivedi et al., 2021 ] to decouple the structural (node\n",
    "features) and positional (node’s position within the graph) representations within GNNs, allowing\n",
    "them to be learned independently and leading to an increased expressivity and performance.\n",
    "To further enhance the expressive power of GNNs, this research project takes inspiration from the\n",
    "Equivariant Message Passing Simplicial Network (EMPSN) architecture [Eijkelboom et al., 2023], a\n",
    "novel approach that combines geometric and topological information on simplicial complexes. Our\n",
    "goal is to develop a generic method that also combines geometric and topological information by\n",
    "improving upon the established LSPE framework. By combining these distinct approaches, we seek\n",
    "to leverage the complementary nature of geometric and topological information in capturing complex\n",
    "graph relationships and enhancing the discriminative capabilities of GNN models. We highlight the\n",
    "following contributions:\n",
    "\n",
    "\n",
    "- We demonstrate that smaller, less complex models experience larger performance gains when\n",
    "utilizing PEs.\n",
    "\n",
    "- We discover a relationship between model complexity and the impact of topological information,\n",
    "where less complex models benefit the most from this additional information.\n",
    "\n",
    "- Injecting topological information through PEs significantly improves performance in non-fully\n",
    "connected settings.\n",
    "\n",
    "- We find that topological information does not necessarily enhance the EGNN model in a\n",
    "fully-connected setting, suggesting that the model learns topology through geometry.\n",
    "\n",
    "- By adapting the LSPE framework to include Geometry, we create a generic method to\n",
    "incorporate both geometric and topological information.\n",
    "\n",
    "- We improve the performance of the standard EGNN model in non-fully connected scenarios.\n",
    "\n",
    "- Our method demonstrates its applicability to different architectures with sufficient model\n",
    "complexity, revealing the limitations of the LSPE framework for less complex models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Related Works\n",
    "\n",
    "### Message Passing Neural Networks\n",
    "\n",
    "Message Passing Neural Networks (MPNNs) have emerged as a popular class of models for graph\n",
    "representation learning. MPNNs allow nodes in a graph to exchange information with their\n",
    "neighbouring nodes through message-passing iterations, enabling the incorporation of local graph\n",
    "structure into node representations. These models typically consist of two key steps: message\n",
    "propagation, where each node aggregates information from its neighbours, and node update, where\n",
    "the aggregated information is used to update the node’s representation. MPNNs have demonstrated\n",
    "strong performance in various graph-related tasks, including node classification, link prediction, and\n",
    "graph generation. Several notable MPNN variants include Graph Convolutional Networks (GCNs)\n",
    "[Kipf and Welling, 2016 ], Graph Attention Networks (GATs) [ Veliˇckovi ́c et al., 2017 ], GraphSAGE\n",
    "[ Hamilton et al., 2017] and Graph Isomorphism Networks (GIN) [Xu et al., 2018 ], each offering\n",
    "unique strategies for message passing and aggregation.\n",
    "\n",
    "![MPNN equations](./images/MPNN%20equations.png)\n",
    "\n",
    "\n",
    "### E(n) Equivariant GNN\n",
    "\n",
    "EGNNs [Satorras et al., 2021] extend MPNNs by relying on graph geometry to improve performance\n",
    "and attain E(n) equivariance. One particular aspect that renders EGNNs highly effective in capturing\n",
    "graph-level topology yet computationally expensive is that they operate best in fully-connected\n",
    "settings with a moderately deep architecture. However, this approach might not always be feasible,\n",
    "as the number of layers needed to learn higher dimensional graph structures scales exponentially,\n",
    "which ultimately renders such architectures intractable for large graphs.\n",
    "\n",
    "### Positional Encodings\n",
    "\n",
    "Positional encodings (PE) are a fundamental concept that significantly influences the effective-\n",
    "ness of many network architectures, including CNNs [Lecun et al., 1998 ], RNNs [Hopfield, 1982 ,\n",
    "Hochreiter and Schmidhuber, 1997], and most recently, Transformers [Vaswani et al., 2017], by\n",
    "providing a means to infuse positional information into deep learning models. However, infer-\n",
    "ring nodes’ positions in any given graph is a non-trivial task due to the absence of a canon-\n",
    "ical positioning method for capturing both local and global information. While GNNs have\n",
    "been shown to outperform traditional algorithms for node classification, subpar performance\n",
    "was demonstrated in [Hu et al., 2020 ] when compared to simple heuristics such as Adamic Adar\n",
    "[Adamic and Adar, 2003] on link prediction tasks [ Liben-Nowell and Kleinberg, 2003 ]. Recent work\n",
    "[ Srinivasan and Ribeiro, 2019, Br ̈uel-Gabrielsson et al., 2022 , Wang et al., 2022] have (empirically)\n",
    "rendered the addition of PE in GNNs crucial in achieving state-of-the-art (SOTA) in graph predic-\n",
    "tion tasks. Several candidates for PE have been proposed, such as Index PE [Murphy et al., 2019 ],\n",
    "Laplacian Eigenvectors [Dwivedi et al., 2022 ] and learnable position-aware embeddings based on\n",
    "random anchor node sets [You et al., 2019]. Another relevant method which this study focuses on\n",
    "involves diffusion-based Random walks [ Bourgain, 1985 , Chung, 1997]. The encodings produced\n",
    "with this method carry significant descriptive power when considering graph topology, as they can\n",
    "effectively capture the graph’s diffusion characteristics [ Topping et al., 2022]. Formally, the RW\n",
    "matrix can be defined over k-steps as :\n",
    "\n",
    "![PE vector](./images/PE-vector.png)\n",
    "\n",
    "### Learnable Structural and Positional Encodings\n",
    "\n",
    "Learnable Structural and Positional Encodings (LSPE) [ Dwivedi et al., 2021 ] have been proposed as\n",
    "an extension to traditional GNN architectures. LSPE combines structural and positional encodings\n",
    "to learn expressive representations that better capture both local and global graph information,\n",
    "resulting in more expressive node embeddings. The structural encoding component focuses on\n",
    "connectivity patterns and neighbourhood information, while the positional encoding component\n",
    "provides topological information into the node representations.\n",
    "Incorporating LSPE into GNNs offers several benefits. It enhances the GNN’s capability to\n",
    "better capture both local and global graph characteristics, enabling better handling of complex\n",
    "graph structures. The learnable nature of the encodings allows the model to adapt and optimize\n",
    "representations for specific tasks, making it suitable for diverse graphs with varying structural and\n",
    "positional properties. The combination of structural and positional encodings provides a richer\n",
    "representation of the graph, manifested through more expressive node embeddings, leading to\n",
    "improved performance in node classification, link prediction, and graph generation tasks.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LSPE Equations](./images/table-3.png)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
